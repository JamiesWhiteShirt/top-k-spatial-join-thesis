General-purpose computing on graphics processors (GPGPU) is the use of a graphics processing unit (GPU) to perform computation that would traditionally be performed on the central processing unit (CPU). The majority of GPUs today are programmable using APIs such as OpenCL and CUDA.\@ Using these APIs, applications can utilize GPUs as coprocessors to execute parts of the application.

GPUs are particularly well-suited for problems that can be expressed as data-parallel computations, which is executing the same program on many data elements in parallel, ideally with high arithmetic intensity. Arithmetic intensity is the ratio of arithmetic operations to memory operations. Graphics processing is the original application, but GPUs have found widespread application in other areas such as machine learning.

This chapter focuses on the architecture of NVIDIA GPUs and the CUDA API.\@

\section{GPU architecture}

GPUs have some significant architectural differences to CPUs, something programmers must be aware of to fully utilize the capabilities of GPUs. The architecture of CPUs is designed for a variety of applications, while the architecture of GPUs is designed primarily for graphics applications. The main differences are that GPU memory is divided into multiple regions with different capacities, bandwidths, latencies and access methods, threads on a GPU can communicate in a number of specialized ways and branching instructions are carried out very differently from CPUs.

GPU architectures evolve over time, which exposes new features to programs and influences the way programs must be designed to optimally utilize the available resources. The features of the GPU architecture are expressed as the compute capability which is a string consisting of a major and minor version number, which at the time of writing ranges from 1.x to 7.x. This chapter describes devices with compute capability 6.x and 7.x, which is available in the most recent generations of hardware at the time of writing.

Unlike the CPU, the GPU dedicates most of its resources to data processing instead of caches and control flow mechanisms. Therefore, the cost of memory accesses and branching is high compared to the cost of data processing such as arithmetic operations. This means that programs with high arithmetic intensity are favored. The individual performance of each thread on a GPU is much lower than the performance of a thread on a CPU, but the GPU makes up for it with the sheer amount of threads that it can execute in parallel.

A GPU contains an array of Streaming Multiprocessors (SMs) and a large shared global memory. The multiprocessors can access the global memory and have an L2 cache that is shared by all multiprocessors. Each multiprocessor has its own L1 cache and on-chip memory that is only accessible to the multiprocessor. The GPU and CPU are connected through an interface such as PCI-e. The bandwidth of the global memory is much higher than the bandwidth between the GPU and CPU.\@ Similarly, the bandwidth of the on-chip memory is much higher than the bandwidth of the global memory.

Work is carried out by creating groups of threads to run a program, and distributing threads between the multiprocessors in groups of 32 parallel threads called warps. Groups of threads may be interdependent and must be scheduled onto the same multiprocessor. Each multiprocessor creates, manages, schedules and executes threads in warps. A warp is the smallest unit of threads, meaning that instantiating a number of threads that is not divisible into warps requires leaving the remainder of threads unutilized. All threads within a warp share a single program but have their own program counters and register states. Warps execute independently, but may share a program and shared memory with other warps in the same multiprocessor, and may also share the program and global memory with warps on other multiprocessors.

Within the multiprocessors, warps are executed with a Single Instruction, Multiple Thread (SIMT) architecture, meaning a warp executes one common instruction at a time on all 32 threads in parallel. Full efficiency is realized when all 32 threads have non-divergent execution paths. When the execution paths diverge, the warp can execute only one common instruction of a subgroup of threads at a time by disabling non-participating threads. Because warps execute independently, the effects of branch divergence only happen within a warp.

The execution of warps within a multiprocessor is efficiently interleaved by warp schedulers, so that an instruction can be executed while the results of other instructions still pending, even interleaving with instructions from other warps. Within a multiprocessor, a context switch from one warp to another has no cost. This is made possible by maintaining the program counters, registers and local memory used by a warp on-chip during the entire lifetime of the warp, even when other warps are executing. Each multiprocessor has a set of registers that are allocated between the warps. The amount of warps that can be executed concurrently by a multiprocessor therefore depends on the requirements of the warps.

Each multiprocessor has on-chip memory with higher bandwidth and much lower latency compared to that of global memory. The local memory is a sequence of 32-bit words distributed into 32 equally sized memory banks. Each memory bank has a bandwidth of 32 bits per clock cycle. A shared memory request for a warp allows each thread to access their own 32-bit word in a single clock cycle provided there is no bank conflict. A memory bank can broadcast a single 32-bit word to all threads requesting the same word, but concurrent access to multiple words within the same bank cannot be serviced in a single clock cycle, which generates a bank conflict. If a bank conflict occurs, the request is split into as many separate conflict-free requests as necessary.

Global memory is accessed via naturally aligned 32-, 64- or 128-byte memory transactions with an important feature called coalescence. When a warp executes an instruction to access global memory, it will attempt to coalesce the memory accesses into one or multiple memory transactions instead of performing a number of transactions per thread. Optimal throughput is generally achieved with the smallest amount of transactions with the least amount of unused words.

\section{CUDA}

CUDA distinguishes two entities, the host and the device. The host has the main memory and the CPU that runs the host program, while the device is a GPU serving as a coprocessor with its own memory. The host can allocate memory on the device and initiate memory transfers between main memory and device memory. The host can use the CUDA API to schedule programs to run on the GPU.\@

The most popular way to use CUDA is by extensions to C or C++ that allows writing programs with parts that run on the host and parts that run on the device. The parts of the program that run on the device are written using the same programming language syntax but use special annotations to utilize parts of the CUDA API, creating a low barrier of entry for C/C++ developers. Feature parity with C and C++ for code that runs on the device is mostly preserved, but there are some notable documented exceptions to which features are available.

The unit of execution in CUDA is a kernel instance. A kernel is a function that can be executed \(N\) times in parallel by \(N\) different CUDA threads by instantiation. When a kernel is instantiated, the caller must specify a number of thread blocks and the number of thread per block that will be used to carry out the execution. The division into thread blocks forms the boundaries for thread cooperation. Each thread block is an independent part of the execution of the kernel, while the threads within each thread block can cooperate.

Threads within the same thread block can cooperate efficiently using a number of mechanisms. Threads can exchange data using memory that is shared by all threads in a group or use barriers and memory fences to synchronize their execution. Because threads are executed in warps, there are also special synchronization mechanisms that allow exchanging data within warps.

Threads and thread blocks are composed into grids. A kernel instance has a grid of thread blocks, and each thread block has a grid of threads. Grids are three-dimensional, but not all dimensions of the grid have to be used, so each element in a grid can be identified using a one-dimensional, two-dimensional or three-dimensional index. The thread index and group index can be used by threads to assign each thread to an individual element in a domain such as a vector, matrix, or volume.

Memory in CUDA is divided into a hierarchy. Each CUDA thread has its own private memory, each thread group has its own shared memory, and all threads have a shared global memory. Global memory is memory that can be accessed both by the host and the kernels. It is the memory with the greatest capacity, typically measured in gigabytes, but is also the memory with the greatest access cost. The host program can freely dynamically allocate global memory, though kernels can do it only to a lesser extent. Shared memory is local to each thread group. It has much lower access cost than global memory, but its capacity is limited by the on-chip memory, commonly between only 48 KB and 92 KB. Shared memory is not dynamically allocated, it is instead statically allocated for each thread group. Private memory has the lowest potential access cost and is also statically allocated. It is primarily stored in registers and is therefore the fastest, but may have to be stored in local memory, which is actually thread-local global memory. Shared memory is stored in the on-chip memory banks. Global memory is stored on the device and is cached by L1 and L2 caches.

Thread blocks are required to execute independently. Thread blocks can be executed in any order, both in parallel and in series. This means that cooperation between thread blocks is not allowed. This programming limitation is key to allow flexibility for the device to find the optimal execution plan to schedule thread blocks across multiprocessors based on its architecture and available resources.

All threads in a thread block are scheduled onto the same multiprocessor. The threads are divided into \(\lceil\frac{T}{W_{size}}\rceil\) warps where \(T\) is the number of threads per block and \(W_{size}\) is the warp size, equal to 32. All warps in a thread block are scheduled onto the same multiprocessor. The execution of each warp is independent and interleaved. If the execution of a warp does not depend on the execution of other warps, it does not have to explicitly synchronize with other warps from the same thread group. Otherwise, memory fences and barriers can be used to synchronize across across warps.

CUDA is an asynchronous API, meaning that the execution on the host and the device may happen asynchronously. The host can schedule tasks such as memory transfers and kernel calls into sequences called streams. Tasks may be executed concurrently and out of order unless the tasks are scheduled in order into the same stream. The host can explicitly synchronize with a stream, for instance to read the result of a kernel execution.

\section{Performance optimization}

The performance of a CUDA application can scale with hardware advancements in multiple ways. The addition of more multiprocessors can increase performance, provided the application can efficiently utilize all multiprocessors, usually by scheduling a sufficient amount of thread blocks. New features added with increased compute capability can be utilized. The clock speed, instruction speed and memory sizes and bandwidths can all be increased. CUDA allows programs to be compiled into an intermediary executable format that allows the hardware and driver to optimize for its architectural features.

A program that parallelizes over a set of threads with little cooperation between threads should generally prefer dividing the set of threads into as many independent thread groups as possible while respecting the warp size.

Programs should utilize local memory for frequently accessed data that is shared by threads in thread groups, with access patterns that avoid bank conflicts.

Requests to global memory should optimize for coalescence. This is achieved by making requests to dense areas of memory and aligning them with naturally aligned 32-, 64- and 128-bit transactions.
